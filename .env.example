# LLM Provider Configuration for LoopEngine
# Copy this file to .env and fill in your values
# WARNING: Never commit .env files containing real API keys!

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Which LLM provider to use: claude | openai | ollama
LLM_PROVIDER=claude

# =============================================================================
# API KEYS (Required for cloud providers)
# =============================================================================
# Claude API key (required if LLM_PROVIDER=claude)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI API key (required if LLM_PROVIDER=openai)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# =============================================================================
# OLLAMA SETTINGS (For local models)
# =============================================================================
# Ollama server URL (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Ollama model to use (default: llama2)
# See available models: https://ollama.ai/library
OLLAMA_MODEL=llama2

# =============================================================================
# LLM BEHAVIOR SETTINGS
# =============================================================================
# Maximum tokens in response (default: 500, range: 1-100000)
LLM_MAX_TOKENS=500

# Response randomness (default: 0.7, range: 0.0-2.0)
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.7

# Request timeout in seconds (default: 30.0)
LLM_TIMEOUT=30.0

# =============================================================================
# CACHING SETTINGS
# =============================================================================
# Cache duration in seconds (default: 300, set to 0 to disable)
BEHAVIOR_CACHE_TTL=300
